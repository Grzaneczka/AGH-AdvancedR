df <- read.csv('piwo.csv', sep = ";", row.names = 1,  header= TRUE)
columns = list(6.0, "min", "max", "max", "max")
standardize(df)
normalize(df)
change_to_stimulants(df, columns)
calculate_weighted_df(df)
calculate_weighted_df(df, c(0.2, 0.2, 0.2, 0.3, 0.2))
calculate_weighted_df(df, c(0.2, 0.2, 0.2, 0.2, 0.2))
calculate_weighted_df(df, c(0.2, 0.2, 0.2, 0.2))
calculate_weighted_df(df, c(0.2, 0.4, 0.2, 0.2))
calculate_weighted_df(df)
library(linearOrderingAndClusterAnalysis)
setwd("C:/Users/karol/OneDrive/Pulpit")
library(linearOrderingAndClusterAnalysis)
df <- read.csv('piwo.csv', sep = ";", row.names = 1,  header= TRUE)
columns = list(6.0, "min", "max", "max", "max")
standardize(df)
normalize(df)
change_to_stimulants(df, columns)
calculate_weighted_df(df)
standardized_sums(df, columns)
print_standardized_sums(df, columns)
library(linearOrderingAndClusterAnalysis)
library(linearOrderingAndClusterAnalysis)
df <- read.csv('piwo.csv', sep = ";", row.names = 1,  header= TRUE)
columns = list(6.0, "min", "max", "max", "max")
calculate_weighted_df(df)
standardized_sums(df, columns)
print_standardized_sums(df, columns)
#standardize(df)
#normalize(df)
#change_to_stimulants(df, columns)
#calculate_weighted_df(df)
#standardized_sums(df, columns)
print_standardized_sums(df, columns, n = 5)
library(linearOrderingAndClusterAnalysis)
library(linearOrderingAndClusterAnalysis)
df <- read.csv('piwo.csv', sep = ";", row.names = 1,  header= TRUE)
columns = list(6.0, "min", "max", "max", "max")
#standardize(df)
#normalize(df)
#change_to_stimulants(df, columns)
#calculate_weighted_df(df)
#standardized_sums(df, columns)
print_standardized_sums(df, columns, n = 5)
print_hellwig(df, columns, n = 5)
setwd("C:/Users/karol/OneDrive/Pulpit/linearOrderingAndClusterAnalysis")
library(linearOrderingAndClusterAnalysis)
setwd("C:/Users/karol/OneDrive/Pulpit")
library(linearOrderingAndClusterAnalysis)
df <- read.csv('piwo.csv', sep = ";", row.names = 1,  header= TRUE)
columns = list(6.0, "min", "max", "max", "max")
#standardize(df)
#normalize(df)
#change_to_stimulants(df, columns)
#calculate_weighted_df(df)
#standardized_sums(df, columns)
print_standardized_sums(df, columns, n = 5)
print_hellwig(df, columns, n = 5)
linear_ordering(df, columns)
library(linearOrderingAndClusterAnalysis)
print_hellwig(df, columns, n = 5)
linear_ordering(df, columns)
#' linear_ordering
#'
#' This function calculate linear ordering
#'
#' @param df input data frame
#' @param columns imput wector about columns
#' @param weight wector or NULL
#' @param n numer of top value (if is null = 3)
#' @return data frame
#' @export
linear_ordering = function(df, columns, weights = NULL){
s = standardized_sums(df, columns, weights = NULL)
h = calculate_hellwig(df, columns, weights = NULL)
h = rank(h)
s = rank(s)
h = h[-1]
s = s[-1]
d <- merge(s, h, by = 'row.names', all = TRUE)
rownames(d) = d[,1]
d = d[-1]
names(d) <- c("standardized_sums", "helwig")
d = dplyr::arrange(d, standardized_sums)
print(d)
}
library(linearOrderingAndClusterAnalysis)
linear_ordering(df, columns)
print_linear_ordering(df, columns, n = 5)
library(linearOrderingAndClusterAnalysis)
print_linear_ordering(df, columns, n = 5)
print_linear_ordering(df, columns, n = 5)
library(linearOrderingAndClusterAnalysis)
install_github("linearOrderingAndClusterAnalysis")
install.packages("devtools")
library(devtools)
install_github("linearOrderingAndClusterAnalysis")
library(linearOrderingAndClusterAnalysis)
detach("package:devtools", unload = TRUE)
print_stat(df)
library(linearOrderingAndClusterAnalysis)
print_stat(df)
library(linearOrderingAndClusterAnalysis)
library(linearOrderingAndClusterAnalysis)
plot_correlation(df)
library(linearOrderingAndClusterAnalysis)
print_stat(df)
plot_correlation(df)
df_scale = standardize(df)
# function to compute total within-cluster sum of square
wss <- function(k) {
cluster::kmeans(df, k, nstart = 10 )$tot.withinss
}
# Compute and plot wss for k = 1 to k = 15
k.values <- 1:15
# extract wss for 2-15 clusters
wss_values <- map_dbl(k.values, wss)
# extract wss for 2-15 clusters
wss_values <- purrr::map_dbl(k.values, wss)
library(cluster, lib.loc = "C:/Program Files/R/R-4.2.0/library")
# function to compute total within-cluster sum of square
wss <- function(k) {
cluster::kmeans(df, k, nstart = 10 )$tot.withinss
}
k2 <- cluster::kmeans(df_scale, centers = 2, nstart = 25)
k2 <- stats::kmeans(df_scale, centers = 2, nstart = 25)
View(k2)
# function to compute total within-cluster sum of square
wss <- function(k) {
stats::kmeans(df, k, nstart = 10 )$tot.withinss
}
# Compute and plot wss for k = 1 to k = 15
k.values <- 1:15
# extract wss for 2-15 clusters
wss_values <- purrr::map_dbl(k.values, wss)
plot(k.values, wss_values,
type="b", pch = 19, frame = FALSE,
xlab="Number of clusters K",
ylab="Total within-clusters sum of squares")
wss_values
print(wss_values[i])
for (i in length(wss_values)){
print(wss_values[i])
}
for (i in 1:length(wss_values)){
print(wss_values[i])
}
for (i in 2:length(wss_values)){
print(wss_values[i] - wss_values[i-1]/wss_values[i])
}
for (i in 2:length(wss_values)){
print((wss_values[i] - wss_values[i-1])/wss_values[i])
}
print((wss_values[i] - wss_values[i-1])/wss_values[i-1])
for (i in 2:length(wss_values)){
print((wss_values[i] - wss_values[i-1])/wss_values[i-1])
}
i = 2
while (((wss_values[i-1] - wss_values[i])/wss_values[i-1]) > 0.30){
i = i + 1
}
center = i
i = 2
while (((wss_values[i-1] - wss_values[i])/wss_values[i-1]) > 0.30){
print(i)
print(((wss_values[i-1] - wss_values[i])/wss_values[i-1]))
i = i + 1
}
i = 2
while (((wss_values[i-1] - wss_values[i])/wss_values[i-1]) < 0.30){
print(i)
print(((wss_values[i-1] - wss_values[i])/wss_values[i-1]))
i = i + 1
}
while (((wss_values[i-1] - wss_values[i])/wss_values[i-1]) > 0.20){
print(i)
print(((wss_values[i-1] - wss_values[i])/wss_values[i-1]))
i = i + 1
}
while (((wss_values[i-1] - wss_values[i])/wss_values[i-1]) > 0.25){
print(i)
print(((wss_values[i-1] - wss_values[i])/wss_values[i-1]))
i = i + 1
}
while (((wss_values[i-1] - wss_values[i])/wss_values[i-1]) > 0.23){
print(i)
print(((wss_values[i-1] - wss_values[i])/wss_values[i-1]))
i = i + 1
}
i = 2
while (((wss_values[i-1] - wss_values[i])/wss_values[i-1]) > 0.23){
print(i)
print(((wss_values[i-1] - wss_values[i])/wss_values[i-1]))
i = i + 1
}
center = i
i = 2
while (((wss_values[i-1] - wss_values[i])/wss_values[i-1]) > 0.25){
print(i)
print(((wss_values[i-1] - wss_values[i])/wss_values[i-1]))
i = i + 1
}
center = i
i = 2
while (((wss_values[i-1] - wss_values[i])/wss_values[i-1]) > 0.25){
print(i)
print(((wss_values[i-1] - wss_values[i])/wss_values[i-1]))
i = i + 1
}
center = i-1
optimasize_k = function(df){
k.values <- 1:15
wss_values <- purrr::map_dbl(k.values, wss)
i = 2
while (((wss_values[i-1] - wss_values[i])/wss_values[i-1]) > 0.25){
print(i)
print(((wss_values[i-1] - wss_values[i])/wss_values[i-1]))
i = i + 1
}
return(i - 1)
}
# function to compute total within-cluster sum of square
wss <- function(df, k) {
stats::kmeans(df, k, nstart = 10 )$tot.withinss
}
colculate_wss = function(df, k){
k.values <- 1:15
wss_values <- c()
for (i in k.values){
wss_values[i] <- wss(df, i)
}
return(wss_values)
}
colculate_wss = function(df){
k.values <- 1:15
wss_values <- c()
for (i in k.values){
wss_values[i] <- wss(df, i)
}
return(wss_values)
}
colculate_wss(df)
optimasize_k = function(df){
wws_values <- colculate_wss(df)
i = 2
while (((wss_values[i-1] - wss_values[i])/wss_values[i-1]) > 0.25){
print(i)
print(((wss_values[i-1] - wss_values[i])/wss_values[i-1]))
i = i + 1
}
return(i - 1)
}
optimasize_k(df)
optimasize_k(df)
optimasize_k = function(df){
wws_values <- colculate_wss(df)
i = 2
while (((wss_values[i-1] - wss_values[i])/wss_values[i-1]) > 0.25){
i = i + 1
}
return(i - 1)
}
optimasize_k(df)
kmeans_print = function(df){
df_scale = standardize(df)
k <- optimasize_k(df_scale)
km <- stats::kmeans(df, k, nstart = 10)
cat("Ten zbiór danych najlepiej podzielić na ", k, "klastrów /n")
}
kmeans_print(df)
kmeans_print = function(df){
df_scale = standardize(df)
k <- optimasize_k(df_scale)
km <- stats::kmeans(df, k, nstart = 10)
cat("Ten zbiór danych najlepiej podzielić na", k, "klastrów /n")
}
kmeans_print(df)
kmeans_print = function(df){
df_scale = standardize(df)
k <- optimasize_k(df_scale)
km <- stats::kmeans(df, k, nstart = 10)
cat("Ten zbiór danych najlepiej podzielić na", k, "klastrów \n")
}
kmeans_print(df)
km <- stats::kmeans(df, k, nstart = 10)
df_scale = standardize(df)
k <- optimasize_k(df_scale)
km <- stats::kmeans(df, k, nstart = 10)
print(km$centers)
kmeans_print = function(df){
df_scale = standardize(df)
k <- optimasize_k(df_scale)
km <- stats::kmeans(df, k, nstart = 10)
cat("Ten zbiór danych najlepiej podzielić na", k, "klastrów \n")
cat("Centroidy klastrów: \n")
print(km$centers)
cat("\n \n")
print(km$cluster)
}
kmeans_print(df)
kmeans_print = function(df){
df_scale = standardize(df)
k <- optimasize_k(df_scale)
km <- stats::kmeans(df, k, nstart = 10)
cat("Ten zbiór danych najlepiej podzielić na", k, "klastrów \n")
cat("Centroidy klastrów: \n")
print(km$centers)
cat("\n \n")
print(km$cluster)
df$grup <- km$cluset
print(df)
}
kmeans_print(df)
df['grup'] <- km$cluset
print(df)
df['grup'] = km$cluset
print(df)
kmeans_print = function(df){
df_scale = standardize(df)
k <- optimasize_k(df_scale)
km <- stats::kmeans(df, k, nstart = 10)
cat("Ten zbiór danych najlepiej podzielić na", k, "klastrów \n")
cat("Centroidy klastrów: \n")
print(km$centers)
cat("\n \n")
print(km$cluster)
df['grup'] = km$cluset
print(df)
}
kmeans_print(df)
df[,ncol(df) + 1] = km$cluset
print(df)
df['grup'] <- 1
df
df[,ncol(df) + 1] <- km$cluset
print(df)
df_scale = standardize(df)
k <- optimasize_k(df_scale)
km <- stats::kmeans(df, k, nstart = 10)
cat("Ten zbiór danych najlepiej podzielić na", k, "klastrów \n")
cat("Centroidy klastrów: \n")
print(km$centers)
cat("\n \n")
print(km$cluster)
df['grup'] <- km$cluset
print(df)
a <- km$cluster
df['grup'] <- a
df
print(df)
kmeans_print = function(df){
df_scale = standardize(df)
k <- optimasize_k(df_scale)
km <- stats::kmeans(df, k, nstart = 10)
cat("Ten zbiór danych najlepiej podzielić na", k, "klastrów \n")
cat("Centroidy klastrów: \n")
print(km$centers)
cat("\n \n")
print(km$cluster)
a <- km$cluster
df['grup'] <- a
print(df)
}
kmeans_print(df)
library(linearOrderingAndClusterAnalysis)
df <- read.csv('piwo.csv', sep = ";", row.names = 1,  header= TRUE)
columns = list(6.0, "min", "max", "max", "max")
# function to compute total within-cluster sum of square
wss <- function(df, k) {
stats::kmeans(df, k, nstart = 10 )$tot.withinss
}
colculate_wss = function(df){
k.values <- 1:15
wss_values <- c()
for (i in k.values){
wss_values[i] <- wss(df, i)
}
return(wss_values)
}
optimasize_k = function(df){
wws_values <- colculate_wss(df)
i = 2
while (((wss_values[i-1] - wss_values[i])/wss_values[i-1]) > 0.25){
i = i + 1
}
return(i - 1)
}
kmeans_print = function(df){
df_scale = standardize(df)
k <- optimasize_k(df_scale)
km <- stats::kmeans(df, k, nstart = 10)
cat("Ten zbiór danych najlepiej podzielić na", k, "klastrów \n")
cat("Centroidy klastrów: \n")
print(km$centers)
cat("\n \n")
print(km$cluster)
a <- km$cluster
df['grup'] <- a
print(df)
}
df
kmeans_print(df)
optimasize_k = function(df){
wss_values <- colculate_wss(df)
i = 2
while (((wss_values[i-1] - wss_values[i])/wss_values[i-1]) > 0.25){
i = i + 1
}
return(i - 1)
}
kmeans_print = function(df){
df_scale = standardize(df)
k <- optimasize_k(df_scale)
km <- stats::kmeans(df, k, nstart = 10)
cat("Ten zbiór danych najlepiej podzielić na", k, "klastrów \n")
cat("Centroidy klastrów: \n")
print(km$centers)
cat("\n \n")
print(km$cluster)
a <- km$cluster
df['grup'] <- a
return(df)
}
kmeans_print(df)
kmeans_print = function(df){
df_scale = standardize(df)
k <- optimasize_k(df_scale)
km <- stats::kmeans(df, k, nstart = 10)
cat("Ten zbiór danych najlepiej podzielić na", k, "klastrów \n")
cat("Centroidy klastrów: \n")
print(km$centers)
cat("\n \n")
print(km$cluster)
a <- km$cluster
df['grup'] <- a
return(df['grup'])
}
kmeans_print(df)
kmeans_print = function(df){
df_scale = standardize(df)
k <- optimasize_k(df_scale)
km <- stats::kmeans(df, k, nstart = 10)
cat("Ten zbiór danych najlepiej podzielić na", k, "klastrów \n")
cat("Centroidy klastrów: \n")
print(km$centers)
cat("\n \n")
print(km$cluster)
a <- km$cluster
df['grup'] <- a
return(dplyr::arrange(df['grup'], grup))
}
kmeans_print(df)
kmeans_print = function(df){
df_scale = standardize(df)
k <- optimasize_k(df_scale)
km <- stats::kmeans(df, k, nstart = 10)
a <- km$cluster
df['grup'] <- a
cat("Ten zbiór danych najlepiej podzielić na", k, "klastrów \n \n")
cat("Centroidy klastrów: \n \n")
print(km$centers)
cat("\n \n Elementy w grupach \n \n")
print(dplyr::arrange(df['grup'], grup))
}
kmeans_print(df)
library(linearOrderingAndClusterAnalysis)
library(linearOrderingAndClusterAnalysis)
df <- read.csv('piwo.csv', sep = ";", row.names = 1,  header= TRUE)
columns = list(6.0, "min", "max", "max", "max")
print_linear_ordering(df, columns)
print_stat(df)
plot_correlation(df)
plot_optimum_k(df)
optimasize_k(df)
df_scale = standardize(df)
k <- optimasize_k(df_scale)
km <- stats::kmeans(df, k, nstart = 10)
factoextra::fviz_cluster(km, data=df)
install.packages("factoextra")
install.packages("factoextra")
factoextra::fviz_cluster(km, data=df)
km <- stats::kmeans(df, 4, nstart = 10)
factoextra::fviz_cluster(km, data=df)
factoextra::fviz_cluster(km, data=df)
library(linearOrderingAndClusterAnalysis)
library(linearOrderingAndClusterAnalysis)
df <- read.csv('piwo.csv', sep = ";", row.names = 1,  header= TRUE)
columns = list(6.0, "min", "max", "max", "max")
# --- SUMMARY ----
summary = function(df, columns, weitght = NULL, n = NULL){
print_stat(df)
plot_correlation(df)
print_linear_ordering(df, columns)
kmeans_print(df)
plot_optimum_k()
plot_kmeans(df)
}
summary(df, columns)
# --- SUMMARY ----
summary = function(df, columns, weitght = NULL, n = NULL){
print_stat(df)
plot_correlation(df)
print_linear_ordering(df, columns)
kmeans_print(df)
plot_optimum_k(df)
plot_kmeans(df)
}
summary(df, columns)
library(linearOrderingAndClusterAnalysis)
library(linearOrderingAndClusterAnalysis)
library(linearOrderingAndClusterAnalysis)
